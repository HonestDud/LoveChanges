# LoveChanges

import praw
import os
import csv
import re
import enchant
from supabase_py import create_client, Client

# create the objects from the imported modules


# reddit api login
reddit = praw.Reddit(client_id='Fill',
                     client_secret='fill',
                     username='LoveChangesbotv1',
                     password='iamwordbotforlove',
                     user_agent='xxx')                   
         
  # Supabase client for database interaction       
 url: str = os.environ.get("SUPABASE_URL")
 key: str = os.environ.get("SUPABASE_KEY")
 supabase: Client = create_client(url, key)

   # Storage and ID of Organization for comments
comments = supabase.table("comment_ids").select("*").execute()
  comments_already_responded_to = [comment['id'] for comment in comments.get("data", [])]

    #Changing comments into an easier to read and reply function for the bot. Enchances performance.
 def clean_string(raw_string):
    cleaned_string = raw_string.lower()
    cleaned_string = re.sub(r'[^A-Za-z0-9 ]+', '', cleaned_string)
    return cleaned_string


# the subreddits you want your bot to live on
subreddit = reddit.subreddit('xxxx')

# look for phrase and reply appropriately/Ensures no repeating comments through supabase 
for comment in subreddit.stream.comments():
    if keyphrase in comment.body.lower() and comment.id not in comments_already_responded_to:
       comment.reply ( List with corresponsing keyphrase chosen response at random )
       data = supabase.table("comment_ids").insert({"id":comment.id, "comment.body}).execute()
            
            else:
                reply = 'You are loved.You deserve the best life has to offer you!'
                comment.reply(reply)
                print('posted')
        except:
            print('too frequent')



  class Redditbot:
    def __init__(self, ResponseL.xlsx):
      self.response_list = [ResponseL.xlsx]

      if len(db) == 0:
        with open(ResponseL.xlsx) as csv_file: 
          csv_reader = csv.reader(csv_file, delimiter=",")
          for row in csv_reader:
            self.response_list.append({
              'phrase': clean_string(row[0]),
              'reply': row[1]
            })
      db['response_list'] = self.response_list
    else:
        print("Pulling from DB")
        self.response_list = db['response_list']
def find_match(self, comment):
        for i, dictionary in enumerate(self.response_list):
            if dictionary['phrase'] in clean_string(comment.body):
                if self.cooled_down(i):
                    self.make_reply(i, comment)
    
  def cooled_down(self, i):
        dictionary = self.response_list[i]
        if 'last_posted' not in dictionary.keys():
            # Means we have never posted on this phrase!
            return True
        else:
            now = datetime.now()
            duration = now - datetime.fromtimestamp(dictionary['last_posted'])
            duration_seconds = duration.total_seconds()
            hours = divmod(duration_seconds, 3600)[0]
            if hours >= 24:
                return True
            else:
                print(f"Couldn't post {dictionary['phrase']} Cool Down time: {24 - hours}")
        
        return False

    def make_reply(self, i, comment):
        dictionary = self.response_list[i]
        try:
            comment.reply(dictionary['reply'])
            print(comment.body)
            print(dictionary['phrase'])
            print(dictionary['reply'])
            # Might want to sleep after posting!
            time.sleep(60 * 60 * 3)
        except Exception as e:
            print(e)

        now = datetime.now()
        self.response_list[i]['last_posted'] = now.timestamp()
        db['response_list'] = self.response_list

# Warning clears all your posted times!
# Use if you want to changes phrases replies
#db.clear()
keep_alive()
bot = RedditBot("pairs.csv")
subreddit = reddit.subreddit("all")
for comment in subreddit.stream.comments(skip_existing=True):
    bot.find_match(comment)









            
             
        
        
